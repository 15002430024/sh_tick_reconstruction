# -*- coding: utf-8 -*-
"""
ä¸Šäº¤æ‰€é€ç¬”æ•°æ®æ‹†è§£ - æ‰¹é‡å¤„ç†æ¨¡å—

å®ç°å•æ—¥å…¨å¸‚åœºæ•°æ®çš„æ‰¹é‡å¤„ç†:
- process_daily_data(): å¤„ç†å•æ—¥å…¨å¸‚åœºæ•°æ®
- check_bizindex_continuity(): BizIndex è¿ç»­æ€§æ£€æŸ¥

å¯¹åº”éœ€æ±‚æ–‡æ¡£: SH_Tick_Data_Reconstruction_Spec v1.8
å¯¹åº”è½åœ°è®¡åˆ’: Phase 3.2
"""

import os
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

import polars as pl

from .reconstructor import reconstruct_sh_tick_data
from .schema import (
    create_order_dataframe,
    create_trade_dataframe,
    write_order_parquet,
    write_trade_parquet,
    SH_ORDER_SCHEMA_PYARROW,
    SH_TRADE_SCHEMA_PYARROW,
)


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# ============================================================================
# æ‰¹é‡å¤„ç†ä¸»å‡½æ•°
# ============================================================================

def process_daily_data(
    date: str,
    input_path: str,
    output_path: str,
    validate_output: bool = True,
    progress_callback: Optional[callable] = None
) -> Dict[str, Any]:
    """
    å¤„ç†å•æ—¥å…¨å¸‚åœºæ•°æ®
    
    å°†ä¸Šäº¤æ‰€ auction_tick_merged_data æ··åˆæ•°æ®æµæ‹†è§£è¿˜åŸä¸º:
    - {date}_sh_order_data.parquet (å§”æ‰˜è¡¨)
    - {date}_sh_trade_data.parquet (æˆäº¤è¡¨)
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ YYYYMMDD (å¦‚ '20260126')
        input_path: è¾“å…¥ Parquet æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºç›®å½•è·¯å¾„
        validate_output: æ˜¯å¦éªŒè¯è¾“å‡º schema
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(security_id, current, total)
    
    Returns:
        Dict[str, Any]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            - total_securities: å¤„ç†çš„è‚¡ç¥¨æ•°é‡
            - total_orders: è¾“å‡ºçš„å§”æ‰˜è®°å½•æ•°
            - total_trades: è¾“å‡ºçš„æˆäº¤è®°å½•æ•°
            - new_orders: æ–°å¢å§”æ‰˜æ•°é‡
            - cancel_orders: æ’¤å•æ•°é‡
            - taker_orders: ä¸»åŠ¨å§”æ‰˜æ•°é‡ (IsAggressive=True)
            - maker_orders: è¢«åŠ¨å§”æ‰˜æ•°é‡ (IsAggressive=False)
            - processing_time_seconds: å¤„ç†è€—æ—¶
            - output_files: è¾“å‡ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    
    Raises:
        FileNotFoundError: å¦‚æœè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: å¦‚æœæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®
    
    Examples:
        >>> stats = process_daily_data(
        ...     date='20260126',
        ...     input_path='/data/raw/auction_tick_merged_data_20260126.parquet',
        ...     output_path='/data/processed/'
        ... )
        >>> print(f"å¤„ç† {stats['total_securities']} åªè‚¡ç¥¨")
        >>> print(f"è¾“å‡º {stats['total_orders']} æ¡å§”æ‰˜, {stats['total_trades']} æ¡æˆäº¤")
    
    Note:
        â­ å…³é”®çº¦æŸ:
        1. è¾“å‡ºæ–‡ä»¶å‘½åå¿…é¡»ä¸º {date}_sh_order_data.parquet / {date}_sh_trade_data.parquet
        2. è¾“å‡ºå¿…é¡»æŒ‰ (SecurityID, TickTime, BizIndex) æ’åº
        3. IsAggressive å¿…é¡»ä¸º nullable bool ç±»å‹
        4. æ‰€æœ‰è®°å½•å¿…é¡»åŒ…å« SecurityID å­—æ®µ
    """
    start_time = datetime.now()
    
    # éªŒè¯æ—¥æœŸæ ¼å¼
    if len(date) != 8 or not date.isdigit():
        raise ValueError(f"æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸º YYYYMMDD: {date}")
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_path).mkdir(parents=True, exist_ok=True)
    
    logger.info(f"å¼€å§‹å¤„ç†æ—¥æœŸ {date} çš„æ•°æ®")
    logger.info(f"è¾“å…¥æ–‡ä»¶: {input_path}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_path}")
    
    # =========================================================================
    # Step 1: è¯»å–æ•°æ®
    # =========================================================================
    logger.info("Step 1: è¯»å–è¾“å…¥æ•°æ®...")
    df = pl.read_parquet(input_path)
    logger.info(f"  è¯»å–å®Œæˆ: {len(df):,} è¡Œ")
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
    security_ids = df['SecurityID'].unique().sort().to_list()
    total_securities = len(security_ids)
    logger.info(f"  å…± {total_securities} åªè‚¡ç¥¨")
    
    # =========================================================================
    # Step 2: æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†
    # =========================================================================
    logger.info("Step 2: æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†...")
    
    all_orders: List[Dict[str, Any]] = []
    all_trades: List[Dict[str, Any]] = []
    
    for i, security_id in enumerate(security_ids):
        # è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback(security_id, i + 1, total_securities)
        
        # è¿‡æ»¤å•åªè‚¡ç¥¨æ•°æ®
        group_df = df.filter(pl.col('SecurityID') == security_id)
        
        # è°ƒç”¨æ ¸å¿ƒé‡å»ºå‡½æ•°
        orders, trades = reconstruct_sh_tick_data(group_df, security_id)
        
        # ç´¯åŠ ç»“æœ
        all_orders.extend(orders)
        all_trades.extend(trades)
        
        # æ—¥å¿— (æ¯ 100 åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡)
        if (i + 1) % 100 == 0 or (i + 1) == total_securities:
            logger.info(f"  å·²å¤„ç† {i + 1}/{total_securities} åªè‚¡ç¥¨")
    
    logger.info(f"  å¤„ç†å®Œæˆ: {len(all_orders):,} æ¡å§”æ‰˜, {len(all_trades):,} æ¡æˆäº¤")
    
    # =========================================================================
    # Step 3: åˆ›å»º DataFrame å¹¶å…¨å¸‚åœºæ’åº
    # =========================================================================
    logger.info("Step 3: åˆ›å»º DataFrame å¹¶æ’åº...")
    
    # åˆ›å»º DataFrame (ä½¿ç”¨ schema è¾…åŠ©å‡½æ•°)
    orders_df = create_order_dataframe(all_orders)
    trades_df = create_trade_dataframe(all_trades)
    
    # å…¨å¸‚åœºæ’åº (SecurityID, TickTime, BizIndex)
    # æ³¨æ„: write_order_parquet å’Œ write_trade_parquet ä¼šè‡ªåŠ¨æ’åºï¼Œ
    # è¿™é‡Œæå‰æ’åºæ˜¯ä¸ºäº†ç»Ÿè®¡éªŒè¯
    orders_df = orders_df.sort(['SecurityID', 'TickTime', 'BizIndex'])
    trades_df = trades_df.sort(['SecurityID', 'TickTime', 'BizIndex'])
    
    logger.info(f"  å§”æ‰˜è¡¨: {orders_df.shape[0]:,} è¡Œ, {orders_df.shape[1]} åˆ—")
    logger.info(f"  æˆäº¤è¡¨: {trades_df.shape[0]:,} è¡Œ, {trades_df.shape[1]} åˆ—")
    
    # =========================================================================
    # Step 4: è¾“å‡º Parquet
    # =========================================================================
    logger.info("Step 4: è¾“å‡º Parquet æ–‡ä»¶...")
    
    # æ–‡ä»¶å‘½åè§„èŒƒ: {date}_sh_order_data.parquet / {date}_sh_trade_data.parquet
    order_file = os.path.join(output_path, f"{date}_sh_order_data.parquet")
    trade_file = os.path.join(output_path, f"{date}_sh_trade_data.parquet")
    
    # å†™å…¥æ–‡ä»¶ (åŒ…å« schema éªŒè¯å’Œæ’åº)
    write_order_parquet(orders_df, order_file, validate=validate_output)
    write_trade_parquet(trades_df, trade_file, validate=validate_output)
    
    logger.info(f"  å§”æ‰˜æ–‡ä»¶: {order_file}")
    logger.info(f"  æˆäº¤æ–‡ä»¶: {trade_file}")
    
    # =========================================================================
    # Step 5: ç»Ÿè®¡ä¿¡æ¯
    # =========================================================================
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    
    # è®¡ç®—ç»Ÿè®¡
    new_orders = orders_df.filter(pl.col('OrdType') == 'New').shape[0]
    cancel_orders = orders_df.filter(pl.col('OrdType') == 'Cancel').shape[0]
    
    # IsAggressive ç»Ÿè®¡ (åªç»Ÿè®¡ New è®¢å•)
    new_orders_df = orders_df.filter(pl.col('OrdType') == 'New')
    taker_orders = new_orders_df.filter(pl.col('IsAggressive') == True).shape[0]
    maker_orders = new_orders_df.filter(pl.col('IsAggressive') == False).shape[0]
    
    stats = {
        'date': date,
        'total_securities': total_securities,
        'total_orders': len(all_orders),
        'total_trades': len(all_trades),
        'new_orders': new_orders,
        'cancel_orders': cancel_orders,
        'taker_orders': taker_orders,
        'maker_orders': maker_orders,
        'processing_time_seconds': round(processing_time, 2),
        'output_files': [order_file, trade_file],
    }
    
    logger.info("=" * 60)
    logger.info(f"å¤„ç†å®Œæˆ! æ—¥æœŸ: {date}")
    logger.info(f"  è‚¡ç¥¨æ•°: {stats['total_securities']:,}")
    logger.info(f"  å§”æ‰˜æ•°: {stats['total_orders']:,} (New: {new_orders:,}, Cancel: {cancel_orders:,})")
    logger.info(f"  æˆäº¤æ•°: {stats['total_trades']:,}")
    logger.info(f"  Taker: {taker_orders:,}, Maker: {maker_orders:,}")
    logger.info(f"  è€—æ—¶: {processing_time:.2f} ç§’")
    logger.info("=" * 60)
    
    return stats


# ============================================================================
# BizIndex è¿ç»­æ€§æ£€æŸ¥
# ============================================================================

def check_bizindex_continuity(
    df: pl.DataFrame,
    security_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ£€æŸ¥ BizIndex è¿ç»­æ€§ï¼Œæ£€æµ‹è·³å·
    
    Args:
        df: åŒ…å« BizIndex åˆ—çš„ DataFrame
        security_id: å¯é€‰çš„è‚¡ç¥¨ä»£ç  (ç”¨äºæ—¥å¿—)
    
    Returns:
        Dict[str, Any]: æ£€æŸ¥ç»“æœ
            - is_continuous: æ˜¯å¦è¿ç»­
            - total_records: æ€»è®°å½•æ•°
            - min_bizindex: æœ€å° BizIndex
            - max_bizindex: æœ€å¤§ BizIndex
            - expected_count: æœŸæœ›è®°å½•æ•° (max - min + 1)
            - gap_count: è·³å·æ•°é‡
            - gaps: è·³å·åˆ—è¡¨ (å‰ 10 ä¸ª)
    
    Examples:
        >>> result = check_bizindex_continuity(df, '600519')
        >>> if not result['is_continuous']:
        ...     print(f"å‘ç° {result['gap_count']} ä¸ªè·³å·")
    """
    if 'BizIndex' not in df.columns:
        raise ValueError("DataFrame ç¼ºå°‘ BizIndex åˆ—")
    
    if len(df) == 0:
        return {
            'is_continuous': True,
            'total_records': 0,
            'min_bizindex': None,
            'max_bizindex': None,
            'expected_count': 0,
            'gap_count': 0,
            'gaps': [],
        }
    
    # æ’åºå¹¶è·å– BizIndex åˆ—è¡¨
    biz_indices = df.sort('BizIndex')['BizIndex'].to_list()
    
    min_idx = biz_indices[0]
    max_idx = biz_indices[-1]
    expected_count = max_idx - min_idx + 1
    actual_count = len(biz_indices)
    gap_count = expected_count - actual_count
    
    # æŸ¥æ‰¾å…·ä½“è·³å·
    gaps = []
    if gap_count > 0:
        biz_set = set(biz_indices)
        for i in range(min_idx, max_idx + 1):
            if i not in biz_set:
                gaps.append(i)
                if len(gaps) >= 10:  # åªè®°å½•å‰ 10 ä¸ª
                    break
    
    result = {
        'is_continuous': gap_count == 0,
        'total_records': actual_count,
        'min_bizindex': min_idx,
        'max_bizindex': max_idx,
        'expected_count': expected_count,
        'gap_count': gap_count,
        'gaps': gaps,
    }
    
    # æ—¥å¿—å‘Šè­¦
    if not result['is_continuous']:
        sec_info = f" [{security_id}]" if security_id else ""
        logger.warning(
            f"BizIndex è¿ç»­æ€§æ£€æŸ¥{sec_info}: å‘ç° {gap_count} ä¸ªè·³å·"
            f" (èŒƒå›´ {min_idx}-{max_idx}, å®é™… {actual_count} æ¡)"
        )
        if gaps:
            logger.warning(f"  è·³å·ç¤ºä¾‹: {gaps[:5]}...")
    
    return result


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def get_output_file_paths(date: str, output_path: str) -> Tuple[str, str]:
    """
    è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)
        output_path: è¾“å‡ºç›®å½•
    
    Returns:
        Tuple[str, str]: (å§”æ‰˜æ–‡ä»¶è·¯å¾„, æˆäº¤æ–‡ä»¶è·¯å¾„)
    """
    order_file = os.path.join(output_path, f"{date}_sh_order_data.parquet")
    trade_file = os.path.join(output_path, f"{date}_sh_trade_data.parquet")
    return order_file, trade_file


def validate_date_format(date: str) -> bool:
    """
    éªŒè¯æ—¥æœŸæ ¼å¼æ˜¯å¦ä¸º YYYYMMDD
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²
    
    Returns:
        bool: True if æ ¼å¼æ­£ç¡®
    """
    if len(date) != 8 or not date.isdigit():
        return False
    
    try:
        year = int(date[:4])
        month = int(date[4:6])
        day = int(date[6:8])
        
        if year < 1990 or year > 2100:
            return False
        if month < 1 or month > 12:
            return False
        if day < 1 or day > 31:
            return False
        
        return True
    except ValueError:
        return False


# ============================================================================
# æ¨¡å—æµ‹è¯•
# ============================================================================

if __name__ == '__main__':
    import tempfile
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 70)
    print("Phase 3.2: æ‰¹é‡å¤„ç†å…¥å£æµ‹è¯•")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        'BizIndex': [100, 101, 102, 200, 201, 300],
        'TickTime': [93000000, 93000100, 93000200, 93001000, 93001100, 93002000],
        'Type': ['T', 'A', 'D', 'T', 'A', 'A'],
        'BuyOrderNO': [1001, 1001, 1001, 1002, 1002, 0],
        'SellOrderNO': [2001, 0, 0, 2002, 0, 3001],
        'Price': [50.0, 50.5, 0, 60.0, 60.5, 45.0],
        'Qty': [500, 300, 200, 600, 400, 2000],
        'TradeMoney': [25000.0, 0, 0, 36000.0, 0, 0],
        'TickBSFlag': ['B', 'B', 'B', 'B', 'B', 'S'],
        'SecurityID': ['600519', '600519', '600519', '600519', '600519', '000001'],
    }
    
    df = pl.DataFrame(test_data)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as tmpdir:
        # ä¿å­˜æµ‹è¯•è¾“å…¥æ–‡ä»¶
        input_file = os.path.join(tmpdir, 'test_input.parquet')
        df.write_parquet(input_file)
        
        print(f"\nğŸ“‹ æµ‹è¯•æ•°æ®: {len(df)} è¡Œ, 2 åªè‚¡ç¥¨")
        
        # è¿›åº¦å›è°ƒ
        def progress(sec_id, current, total):
            print(f"  å¤„ç†: {sec_id} ({current}/{total})")
        
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        stats = process_daily_data(
            date='20260126',
            input_path=input_file,
            output_path=tmpdir,
            progress_callback=progress
        )
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        order_file, trade_file = get_output_file_paths('20260126', tmpdir)
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶éªŒè¯:")
        assert os.path.exists(order_file), "å§”æ‰˜æ–‡ä»¶ä¸å­˜åœ¨"
        assert os.path.exists(trade_file), "æˆäº¤æ–‡ä»¶ä¸å­˜åœ¨"
        print(f"  âœ… å§”æ‰˜æ–‡ä»¶å­˜åœ¨: {os.path.basename(order_file)}")
        print(f"  âœ… æˆäº¤æ–‡ä»¶å­˜åœ¨: {os.path.basename(trade_file)}")
        
        # è¯»å–å¹¶éªŒè¯å†…å®¹
        orders_df = pl.read_parquet(order_file)
        trades_df = pl.read_parquet(trade_file)
        
        print(f"\nğŸ“‹ è¾“å‡ºå†…å®¹:")
        print(f"  å§”æ‰˜: {len(orders_df)} æ¡")
        print(f"  æˆäº¤: {len(trades_df)} æ¡")
        
        # éªŒè¯æ’åº
        assert orders_df['SecurityID'].to_list() == sorted(orders_df['SecurityID'].to_list()), \
            "å§”æ‰˜æœªæŒ‰ SecurityID æ’åº"
        print(f"  âœ… å§”æ‰˜æŒ‰ (SecurityID, TickTime, BizIndex) æ’åº")
        
        # éªŒè¯ IsAggressive nullable
        assert orders_df['IsAggressive'].null_count() >= 0, "IsAggressive åº”æ”¯æŒ null"
        cancel_orders = orders_df.filter(pl.col('OrdType') == 'Cancel')
        if len(cancel_orders) > 0:
            assert cancel_orders['IsAggressive'].null_count() == len(cancel_orders), \
                "æ’¤å•çš„ IsAggressive åº”ä¸º None"
        print(f"  âœ… IsAggressive ä¸º nullable bool")
        
        # éªŒè¯ SecurityID å­˜åœ¨
        assert 'SecurityID' in orders_df.columns
        assert 'SecurityID' in trades_df.columns
        print(f"  âœ… æ‰€æœ‰è®°å½•åŒ…å« SecurityID")
    
    print("\n" + "=" * 70)
    print("âœ… Phase 3.2 æ‰¹é‡å¤„ç†å…¥å£æµ‹è¯•é€šè¿‡!")
    print("=" * 70)
